# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #5 выполнил:
- Малыгин Виктор Александрович
- НМТ-213929
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

## Задание 1
### Измените параметры файла yaml-агента и определить, какие параметры и как влияют на обучение модели.
На данном этапе требуется поэксперементировать с параметрами yaml-агента

#### Как были изменены параметры агента:
```C#
behaviors:
  Economic:
    trainer_type: ppo
    hyperparameters:
      batch_size: 1024
      buffer_size: 10240
      learning_rate: 3.0e-4
      learning_rate_schedule: linear
      beta: 1.0e-2
      epsilon: 0.3
      lambd: 0.92
      num_epoch: 7      
    network_settings:
      normalize: false
      hidden_units: 128
      num_layers: 2
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    checkpoint_interval: 500000
    max_steps: 750000
    time_horizon: 64
    summary_freq: 5000
    self_play:
      save_steps: 20000
      team_change: 100000
      swap_steps: 10000
      play_against_latest_model_ratio: 0.5
      window: 10
```

#### Изображение:
![изображение](https://user-images.githubusercontent.com/61794638/205279593-0ef0e288-bac7-4e5d-b595-89660da21437.png)
#### В итоге получаем: 
![изображение](https://user-images.githubusercontent.com/61794638/205279722-4d08be77-c93b-45f8-906b-2bb806bcc630.png)
![изображение](https://user-images.githubusercontent.com/61794638/205279740-10afabc6-c0ce-481e-b943-d3d29dd21c2b.png)
### Вывод по эксперименту: излом кривой хаотичности системы.
### Параметры yaml-агента, которые могут быть изменены:
`beta` - Сила регуляризации энтропии, которая делает политику «более случайной». Это гарантирует, что агенты должным образом исследуют пространство действия во время обучения. Увеличение этого параметра обеспечит выполнение большего количества случайных действий. Это должно быть скорректировано таким образом, чтобы энтропия (измеряемая с помощью TensorBoard) медленно уменьшалась вместе с увеличением вознаграждения. Если энтропия падает слишком быстро, увеличьте бета. Если энтропия падает слишком медленно, уменьшите beta.
`epsilon` - Влияет на скорость изменения политики во время обучения. Соответствует допустимому порогу расхождения между старой и новой политикой при обновлении градиентного спуска. Установка небольшого значения этого параметра приведет к более стабильным обновлениям, но также замедлит процесс обучения. 
`lambd` - Параметр регуляризации лямбда, используемый при расчете обобщенной оценки преимущества. Это можно рассматривать как то, насколько агент полагается на свою текущую оценку стоимости при вычислении обновленной оценки стоимости. Низкие значения соответствуют большему полаганию на текущую оценку ценности (что может быть высоким смещением), а высокие значения соответствуют большему полаганию на фактические вознаграждения, полученные в среде (что может быть высокой дисперсией). Параметр обеспечивает компромисс между ними, и правильное значение может привести к более стабильному процессу обучения. 
`num_epoch` - Количество проходов через буфер опыта при выполнении оптимизации градиентного спуска. Чем больше размер партии, тем больше это допустимо. Уменьшение этого параметра обеспечит более стабильные обновления за счет более медленного обучения. 
#### Создадим новую конфигурацию агента
```
behaviors:
  Economic:
    trainer_type: ppo
    hyperparameters:
      batch_size: 1024
      buffer_size: 10240
      learning_rate: 3.0e-4
      learning_rate_schedule: linear
      beta: 1.0e-3
      epsilon: 0.1
      lambd: 0.94
      num_epoch: 5      
    network_settings:
      normalize: false
      hidden_units: 128
      num_layers: 2
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    checkpoint_interval: 500000
    max_steps: 750000
    time_horizon: 64
    summary_freq: 5000
    self_play:
      save_steps: 20000
      team_change: 100000
      swap_steps: 10000
      play_against_latest_model_ratio: 0.5
      window: 10
```
![изображение](https://user-images.githubusercontent.com/61794638/205280434-97faeca2-1518-44a2-b658-b50dd0717857.png)
#### Покажем полученный результат:
![изображение](https://user-images.githubusercontent.com/61794638/205280514-6e99f6a3-4772-4782-9e97-72b01afc1708.png)
![изображение](https://user-images.githubusercontent.com/61794638/205281152-6565347d-7274-4101-96df-251354a8b5ca.png)
#### Вывод: получен более предсказуемый результат reward и entropy, система доработала имеющиеся результаты до идеала 
## Задание 2
### Анализ графиков
![изображение](https://user-images.githubusercontent.com/61794638/205281106-ad42b6be-d4bd-4080-a7ca-966bc46e62df.png)
`Синия прямая` - график полученный, относительно обучения, сделанного преподавателем. В данном решении система совершала действия, положительно отражающиеся на обучении и получала одну и ту же награду
`Красная ломанная` - график полученный в первом эксперименте, где мы расставляли рандомные значения. Система обучалась быстро, но хаотичность тала мешать и награда упала.
`Голубая ломанная` - итоговый эксперимент, где мы получили рост награды и развитие системы.


## Выводы
В ходе данной лабораторной работы мы ознакомились с некоторыми новыми инструментами: transorflow - инструмент, позволяющий отображать поведение нейросети на графике. Так же мы использовали ml-agent для решения задач.
